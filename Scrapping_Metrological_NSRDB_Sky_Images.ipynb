{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPR7tWhTTuoe"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import urllib.parse\n",
        "import time\n",
        "\n",
        "API_KEY = \"{{YOUR_API_KEY}}\"\n",
        "EMAIL = \"{{YOUR_EMAIL_ID}}\"\n",
        "BASE_URL = \"https://developer.nrel.gov/api/nsrdb/v2/solar/full-disc-download.json?\"\n",
        "\n",
        "# LOCATIONS; Here a observatory near Denver, Colorado.\n",
        "POINTS = [\n",
        "'5567083'\n",
        "]\n",
        "\n",
        "def main():\n",
        "    # features selected to be scrapped.\n",
        "    input_data = {\n",
        "        'attributes': 'dhi,air_temperature,clearsky_dhi,clearsky_dni,clearsky_ghi,cloud_type,dew_point,dni,fill_flag,ghi,ozone,relative_humidity,solar_zenith_angle,surface_albedo,surface_pressure,total_precipitable_water,wind_direction,wind_speed',\n",
        "        # downloading data for every 60 mins interval\n",
        "        'interval': '60',\n",
        "\n",
        "        'api_key': API_KEY,\n",
        "        'email': EMAIL,\n",
        "    }\n",
        "    for name in ['2021','2022','2023']:\n",
        "        print(f\"Processing name: {name}\")\n",
        "        for id, location_ids in enumerate(POINTS):\n",
        "            input_data['names'] = [name]\n",
        "            input_data['location_ids'] = location_ids\n",
        "            print(f'Making request for point group {id + 1} of {len(POINTS)}...')\n",
        "\n",
        "            if '.csv' in BASE_URL:\n",
        "                url = BASE_URL + urllib.parse.urlencode(data, True)\n",
        "                # Note: CSV format is only supported for single point requests\n",
        "                # Suggest that you might append to a larger data frame\n",
        "                data = pd.read_csv(url)\n",
        "                print(f'Response data (you should replace this print statement with your processing): {data}')\n",
        "                # You can use the following code to write it to a file\n",
        "                # data.to_csv('SingleBigDataPoint.csv')\n",
        "            else:\n",
        "                headers = {\n",
        "                  'x-api-key': API_KEY\n",
        "                }\n",
        "                data = get_response_json_and_handle_errors(requests.post(BASE_URL, input_data, headers=headers))\n",
        "                download_url = data['outputs']['downloadUrl']\n",
        "                # You can do with what you will the download url\n",
        "                print(data['outputs']['message'])\n",
        "                print(f\"Data can be downloaded from this url when ready: {download_url}\")\n",
        "\n",
        "                # Delay for 1 second to prevent rate limiting\n",
        "                time.sleep(1)\n",
        "            print(f'Processed')\n",
        "\n",
        "\n",
        "def get_response_json_and_handle_errors(response: requests.Response) -> dict:\n",
        "    \"\"\"Takes the given response and handles any errors, along with providing\n",
        "    the resulting json\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    response : requests.Response\n",
        "        The response object\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        The resulting json\n",
        "    \"\"\"\n",
        "    if response.status_code != 200:\n",
        "        print(f\"An error has occurred with the server or the request. The request response code/status: {response.status_code} {response.reason}\")\n",
        "        print(f\"The response body: {response.text}\")\n",
        "        exit(1)\n",
        "\n",
        "    try:\n",
        "        response_json = response.json()\n",
        "    except:\n",
        "        print(f\"The response couldn't be parsed as JSON, likely an issue with the server, here is the text: {response.text}\")\n",
        "        exit(1)\n",
        "\n",
        "    if len(response_json['errors']) > 0:\n",
        "        errors = '\\n'.join(response_json['errors'])\n",
        "        print(f\"The request errored out, here are the errors: {errors}\")\n",
        "        exit(1)\n",
        "    return response_json\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mJwFJtC-T4pt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}